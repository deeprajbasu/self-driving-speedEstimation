{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract frames from mp4 files and return list of frames\n",
    "\n",
    "total_frames = 1000#control how many frames to add\n",
    "\n",
    "\n",
    "def getFrames(vidfile):\n",
    "    vid = cv2.VideoCapture(vidfile)\n",
    "    frames = []\n",
    "    endframe = True\n",
    "    t = 0\n",
    "    while endframe:\n",
    "        if t>total_frames:\n",
    "            break\n",
    "        endframe,frame = vid.read()\n",
    "        #print(frame)\n",
    "        if endframe:\n",
    "            frames.append(frame)\n",
    "            #print(\"processing frame: \",t)\n",
    "            t+=1\n",
    "    return frames\n",
    "\n",
    "#read outpub vals for txtfile and make list    \n",
    "def getSpeed(txtfile):\n",
    "    speeds = []\n",
    "    t=0\n",
    "    with open(txtfile) as f:\n",
    "        for line in f :\n",
    "            if t>total_frames:\n",
    "                break\n",
    "            speed = line.rstrip('\\n')\n",
    "            speed = float(speed)\n",
    "            speeds.append(speed)\n",
    "            #print(\"speed at frame: \",t,\"is: \",speed)\n",
    "            t+=1\n",
    "    return speeds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get data from files into lists\n",
    "\n",
    "path_txt = \"data/train.txt\"\n",
    "path_mp4 = \"data/train.mp4\"\n",
    "\n",
    "\n",
    "images_data= getFrames(path_mp4)\n",
    "speed_data = getSpeed(path_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((480, 640, 3), 28.105569)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "images_data[0].shape,speed_data[0]\n",
    "#confirm image and floats in lists\n"
   ]
  },
  {
   "source": [
    "[optical flow](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html)\n",
    "\n",
    "<img src=\"https://opencv-python-tutroals.readthedocs.io/en/latest/_images/optical_flow_basic1.jpg\">\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#generate optical flow output with raw data\n",
    "\n",
    "def compute_dense_optical_flow(prev_image, current_image):\n",
    "\n",
    "  #resize images \n",
    "\n",
    "  scale_percent = 20 # percent of original size\n",
    "  width = int(prev_image.shape[1] * scale_percent / 100)\n",
    "  height = int(prev_image.shape[0] * scale_percent / 100)\n",
    "  dim = (width, height)\n",
    "  \n",
    "  # resize image\n",
    "  prev_image = cv2.resize(prev_image, dim, interpolation = cv2.INTER_AREA)\n",
    "  current_image = cv2.resize(current_image, dim, interpolation = cv2.INTER_AREA) \n",
    "\n",
    "\n",
    "\n",
    "  old_shape = current_image.shape\n",
    "  prev_image_gray = cv2.cvtColor(prev_image, cv2.COLOR_BGR2GRAY)\n",
    "  current_image_gray = cv2.cvtColor(current_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  assert current_image.shape == old_shape\n",
    "  \n",
    "  hsv = np.zeros_like(prev_image)\n",
    "  hsv[..., 1] = 255\n",
    "  flow = None\n",
    "  flow = cv2.calcOpticalFlowFarneback(prev=prev_image_gray,\n",
    "                                      next=current_image_gray, flow=flow,\n",
    "                                      pyr_scale=0.8, levels=15, winsize=5,\n",
    "                                      iterations=10, poly_n=5, poly_sigma=0,\n",
    "                                      flags=10)\n",
    "\n",
    "  mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "  hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "  hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "  return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create optical flow images and list of mean speed\n",
    "#features and labels we will be hitting the model with\n",
    "\n",
    "opticalFlowFrames = []\n",
    "speed_Data_final=[]\n",
    "for i in range(0,len(images_data)-1):#trying to get better optical flow by comparing distant frames,                                                                 #instead of consecutive \n",
    "\n",
    "    image = compute_dense_optical_flow(images_data[i],images_data[i+1])\n",
    "    opticalFlowFrames.append(image)\n",
    "    mean_speed = (speed_data[i] + speed_data[i+1])\n",
    "    label = np.asarray(mean_speed,dtype= np.float32)\n",
    "    speed_Data_final.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see optical flow result\n",
    "img = Image.fromarray(opticalFlowFrames[4], 'RGB')\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train and validation split\n",
    "\n",
    "split_ratio = 0.8\n",
    "train_data = opticalFlowFrames[:int(split_ratio*len(opticalFlowFrames))]\n",
    "train_labels = speed_Data_final[:int(split_ratio*len(speed_Data_final))]\n",
    "\n",
    "val_data = opticalFlowFrames[int(split_ratio*len(opticalFlowFrames)):]\n",
    "val_labels = speed_Data_final[int(split_ratio*len(speed_Data_final)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(800, 200)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "len(train_labels),len(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tensorflow datasets using our features and labels \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data,train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data,val_labels))\n",
    "\n",
    "#batch up our datasets \n",
    "train_dataset=train_dataset.batch(10)\n",
    "val_dataset=val_dataset.batch(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "#create our model\n"
   ]
  },
  {
   "source": [
    "[end to end learning for self driving cars](https://arxiv.org/pdf/1604.07316.pdf)\n",
    "\n",
    "<img src=\"model.png\">\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Conv2D(24,(5,5), strides=2, padding='same',activation='relu',input_shape=(96, 128, 3)))\n",
    "\n",
    "#convolutions\n",
    "model.add(tf.keras.layers.Conv2D(36,(5,5), strides=2, padding='same',activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(48,(5,5), strides=2, padding='same',activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(64,(3,3), strides=1, padding='same',activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(64,(3,3), strides=1, padding='same',activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#dense layers\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_5 (Conv2D)            (None, 48, 64, 24)        1824      \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 24, 32, 36)        21636     \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 12, 16, 48)        43248     \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 12, 16, 64)        27712     \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 12, 16, 64)        36928     \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 12288)             0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 100)               1228900   \n_________________________________________________________________\ndense_5 (Dense)              (None, 50)                5050      \n_________________________________________________________________\ndense_6 (Dense)              (None, 10)                510       \n_________________________________________________________________\ndense_7 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 1,365,819\nTrainable params: 1,365,819\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() #observe our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.MSE,\n",
    "              metrics=['MSE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 96, 128, 3), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.float32, name=None))"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras TensorBoard callback.\n",
    "from datetime import datetime \n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train for 80 steps, validate for 20 steps\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 27s 336ms/step - loss: 278.4848 - MSE: 278.4849 - val_loss: 490.3884 - val_MSE: 490.3884\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 29s 359ms/step - loss: 117.2400 - MSE: 117.2400 - val_loss: 35.4412 - val_MSE: 35.4412\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 32s 395ms/step - loss: 22.7735 - MSE: 22.7735 - val_loss: 13.0399 - val_MSE: 13.0399\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 16s 196ms/step - loss: 23.0792 - MSE: 23.0792 - val_loss: 35.5594 - val_MSE: 35.5594\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 11s 141ms/step - loss: 10.4027 - MSE: 10.4027 - val_loss: 36.1399 - val_MSE: 36.1399\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 11s 131ms/step - loss: 8.4108 - MSE: 8.4108 - val_loss: 35.4950 - val_MSE: 35.4950\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 11s 138ms/step - loss: 7.7273 - MSE: 7.7273 - val_loss: 33.8460 - val_MSE: 33.8460\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 10s 129ms/step - loss: 7.5685 - MSE: 7.5685 - val_loss: 30.9902 - val_MSE: 30.9902\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 11s 137ms/step - loss: 7.8058 - MSE: 7.8058 - val_loss: 27.3612 - val_MSE: 27.3612\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 11s 139ms/step - loss: 8.9676 - MSE: 8.9676 - val_loss: 23.1436 - val_MSE: 23.1436\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6f04a1ebe0>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=10,validation_data=val_dataset,use_multiprocessing=True,callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('comma': conda)",
   "language": "python",
   "name": "python361264bitcommacondab076faaeeb984dccbef60de7201bf3cf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}